{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5604358",
   "metadata": {},
   "source": [
    "# Setup Environment and Paths\n",
    "\n",
    "This cell imports necessary libraries and sets the device (GPU or CPU).  \n",
    "It also defines the file paths to the three pre-trained model checkpoints that will be loaded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e10e647",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# --- Paths to model checkpoints ---\n",
    "identity_encoder_path = r\"DeepExPo/DeepExPo_Weights/DSID_Checkpoints/dsid-source-identity-encoder-cpk.pth\"\n",
    "semantic_encoder_path = r\"DeepExPo/DeepExPo_Weights/DSID_Checkpoints/dsid-target-sementic-encoder-cpk.pth\"\n",
    "MEAF_model_path = r\"DeepExPo/DeepExPo_Weights/DeepExPo_MEAF_weights.pth\"  # add extension if needed\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1726b4",
   "metadata": {},
   "source": [
    "# Define or Import Model Architectures\n",
    "\n",
    "Here we define placeholder PyTorch model classes for the Identity Encoder, Semantic Encoder, and the MEAF Model.  \n",
    "You should replace these with your actual model definitions or imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a9f532",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DSID(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.identity_encoder = ...\n",
    "        self.semantic_encoder = ...\n",
    "\n",
    "    def forward(self, x_id, x_sem):\n",
    "        id_emb = self.identity_encoder(x_id)\n",
    "        sem_emb = self.semantic_encoder(x_sem)\n",
    "        return id_emb, sem_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c907a63",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model Weights\n",
    "\n",
    "This cell loads the saved checkpoint weights into the model instances and sets the models to evaluation mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb40f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "identity_encoder.load_state_dict(torch.load(identity_encoder_path, map_location=device))\n",
    "semantic_encoder.load_state_dict(torch.load(semantic_encoder_path, map_location=device))\n",
    "third_model.load_state_dict(torch.load(third_model_path, map_location=device))\n",
    "\n",
    "identity_encoder.eval()\n",
    "semantic_encoder.eval()\n",
    "third_model.eval()\n",
    "\n",
    "print(\"Models loaded and set to eval mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7f54e",
   "metadata": {},
   "source": [
    "# Define Image Preprocessing Pipeline\n",
    "\n",
    "Defines the transformations applied to input images before feeding them into the models, including resizing, converting to tensors, and normalizing pixel values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deeee04",
   "metadata": {},
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # adjust if needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6b3e8",
   "metadata": {},
   "source": [
    "# Load and Preprocess Input Images\n",
    "\n",
    "Prompts for the identity and reference semantic image paths, loads the images, converts them to RGB, and applies the preprocessing pipeline to prepare them for model input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89106568",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "identity_img_path = input(\"Enter path to identity image: \").strip()\n",
    "semantic_img_path = input(\"Enter path to reference semantic image: \").strip()\n",
    "\n",
    "assert os.path.exists(identity_img_path), f\"Identity image not found: {identity_img_path}\"\n",
    "assert os.path.exists(semantic_img_path), f\"Semantic image not found: {semantic_img_path}\"\n",
    "\n",
    "identity_img = Image.open(identity_img_path).convert(\"RGB\")\n",
    "semantic_img = Image.open(semantic_img_path).convert(\"RGB\")\n",
    "\n",
    "identity_tensor = preprocess(identity_img).unsqueeze(0).to(device)\n",
    "semantic_tensor = preprocess(semantic_img).unsqueeze(0).to(device)\n",
    "\n",
    "print(f\"Images loaded and preprocessed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba01ddd",
   "metadata": {},
   "source": [
    "# Extract Identity and Semantic Embeddings\n",
    "\n",
    "Runs the identity encoder and semantic encoder on the preprocessed images to obtain feature embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f2113",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    identity_emb = identity_encoder(identity_tensor)\n",
    "    semantic_emb = semantic_encoder(semantic_tensor)\n",
    "\n",
    "print(f\"Identity embedding shape: {identity_emb.shape}\")\n",
    "print(f\"Semantic embedding shape: {semantic_emb.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f23c2-cf4a-4d67-ba61-639e0db2d47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "to_tensor = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),            # converts to [0,1] float tensor, shape [C,H,W]\n",
    "])\n",
    "\n",
    "img_id  = Image.open(\"/home/afgan_server/Important_Code/New_ID_Images/fifth_Id/Screenshot 2025-02-14 155404.jpg\").convert(\"RGB\")\n",
    "img_sem = Image.open(\"/home/afgan_server/Important_Code/New_ID_Images/fifth_Id/Screenshot 2025-02-14 155404.jpg\").convert(\"RGB\")\n",
    "\n",
    "x_id  = to_tensor(img_id ).unsqueeze(0).to(device)   # add batch dim -> [1,3,256,256]\n",
    "x_sem = to_tensor(img_sem).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a548d9",
   "metadata": {},
   "source": [
    "# Combine or Process Embeddings\n",
    "\n",
    "Optionally combines the extracted embeddings (for example, concatenation) in preparation for inference by the third model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f422cbe9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = third_model(identity_emb, semantic_emb)  # or pass combined_emb if needed\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176354cc",
   "metadata": {},
   "source": [
    "# Run Inference with Third Model\n",
    "\n",
    "Feeds the embeddings into the third model to generate the output (e.g., synthesized image tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c2f43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tensor_to_pil(tensor):\n",
    "    tensor = tensor.squeeze(0).cpu()\n",
    "    tensor = tensor.clamp(0, 1)  # clip values to [0,1]\n",
    "    tensor = tensor.permute(1, 2, 0).numpy() * 255\n",
    "    tensor = tensor.astype('uint8')\n",
    "    return Image.fromarray(tensor)\n",
    "\n",
    "# Assuming output tensor is normalized [0,1]. Adjust if necessary.\n",
    "output = (output - output.min()) / (output.max() - output.min())  # normalize to [0,1]\n",
    "\n",
    "generated_img = tensor_to_pil(output)\n",
    "output_path = \"generated_output.png\"\n",
    "generated_img.save(output_path)\n",
    "\n",
    "print(f\"Generated image saved to {output_path}\")\n",
    "\n",
    "# Display inline in notebook\n",
    "generated_img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319b5d3-1725-41aa-97c5-6a965a49d80f",
   "metadata": {},
   "source": [
    "# DeepExPo Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602cf0a-ce2d-4018-9f7d-5c950fe4ac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, AutoencoderKL\n",
    "lora_weights_path =\"/home/afgan_server/Important_Code/Old_ID_Fine_Tune/ID_2_fine_tune\"\n",
    "vae = AutoencoderKL.from_pretrained(\"/home/afgan_server/Important_Code/stable_diffusion_base_weights/madebyollinsdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"/home/afgan_server/Important_Code/stable_diffusion_base_weights/stabilityaistable-diffusion-xl-base-1.0\",\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "pipe.load_lora_weights(lora_weights_path)\n",
    "_ = pipe.to(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
